{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Rice Price Collector","text":"<p>Rice Price Collector is a modular Python package designed to automatically download, parse, and extract daily rice price data from the Central Bank of Sri Lanka (CBSL) reports. It aims to build a clean and structured dataset suitable for time-series forecasting, price trend analysis, and data-driven policy research.</p>"},{"location":"#overview","title":"Overview","text":"<p>The system is divided into two main modules:</p>"},{"location":"#1-downloader-rice_price_collectordownloader","title":"1. Downloader (<code>rice_price_collector.downloader</code>)","text":"<p>Handles fetching of daily CBSL PDF reports via asynchronous web scraping using <code>aiohttp</code> and <code>BeautifulSoup</code>. All downloaded PDFs are saved by year in the <code>data/raw/&lt;year&gt;/</code> folder.</p> <p>Key features: - Fast, async PDF downloads from CBSL. - Robust against missing pages and timeouts. - Configurable base URLs and year filters.</p>"},{"location":"#2-parser-rice_price_collectorparser","title":"2. Parser (<code>rice_price_collector.parser</code>)","text":"<p>Extracts structured rice price tables from downloaded PDFs using libraries such as pdfplumber or tabula. It supports both batch and single-file extraction.</p> <p>Key features: - Automatically detects Pettah and Marandagahamula tables. - Handles irregular column spacing and missing values. - Produces CSV/Parquet outputs ready for downstream analysis.</p>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>rice_price_collector/\n\u251c\u2500\u2500 downloader/\n\u2502   \u251c\u2500\u2500 pdf_downloader.py     \u2190 Main async downloader\n\u2502   \u251c\u2500\u2500 config.py             \u2190 CBSL URLs and constants\n\u2502   \u2514\u2500\u2500 __main__.py           \u2190 Entry point: run downloader\n\u2514\u2500\u2500 parser/\n    \u251c\u2500\u2500 batch_extract.py      \u2190 Batch extraction pipeline\n    \u251c\u2500\u2500 parser.py             \u2190 High-level extraction logic\n    \u251c\u2500\u2500 columns.py            \u2190 Column mapping + validation\n    \u251c\u2500\u2500 utils.py              \u2190 Helper functions\n    \u2514\u2500\u2500 extractors/\n        \u2514\u2500\u2500 rice.py           \u2190 Rice table parser\n</code></pre>"},{"location":"about/","title":"About rice_price_collector","text":"<p>rice_price_collector is an open-source Python package for downloading and extracting rice price data from the Central Bank of Sri Lanka (CBSL) PDF reports. It is designed for researchers, analysts, and developers who need programmatic access to historical rice price data for data science, economics, and agricultural studies.</p>"},{"location":"about/#features","title":"Features","text":"<ul> <li>Download CBSL rice price PDF reports for multiple years with a single command.</li> <li>Extract structured data from downloaded PDFs into pandas DataFrames.</li> <li>Clean, scikit-learn-style API for easy use in scripts and notebooks.</li> <li>Fully open-source and extensible.</li> </ul>"},{"location":"about/#typical-use-cases","title":"Typical Use Cases","text":"<ul> <li>Building time series datasets of rice prices for analysis.</li> <li>Automating the collection and cleaning of CBSL rice price data.</li> <li>Integrating rice price data into data science and machine learning workflows.</li> </ul>"},{"location":"about/#project-links","title":"Project Links","text":"<ul> <li>GitHub Repository</li> <li>TestPyPI Package</li> </ul>"},{"location":"about/#license","title":"License","text":"<p>MIT License</p> <p>rice_price_collector is maintained by ChamoChiran and contributors. Contributions and feedback are welcome!</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide walks you through setting up and running the Rice Price Collector package \u2014 from environment setup to running both the downloader and parser modules.</p>"},{"location":"getting-started/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"getting-started/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/ChamoChiran/rice_price_collector.git\ncd rice_price_collector\n</code></pre>"},{"location":"getting-started/#2-create-and-activate-the-environment","title":"2. Create and Activate the Environment","text":"<pre><code>conda env create -f environment.yml\nconda activate rice_price_collector\n</code></pre> <p>The environment installs everything you need: - <code>aiohttp</code> and <code>BeautifulSoup4</code> for asynchronous web scraping - <code>pdfplumber</code> or <code>tabula</code> for table extraction - <code>pandas</code> and <code>numpy</code> for data cleaning</p>"},{"location":"getting-started/#running-the-package","title":"Running the Package","text":""},{"location":"getting-started/#option-1-launch-interactive-menu","title":"Option 1 \u2014 Launch Interactive Menu","text":"<p>Your package includes a launcher script (<code>rice_price_collector/__main__.py</code>) that provides a simple command-line interface to choose a module.</p> <p>Run it like this:</p> <pre><code>python -m rice_price_collector\n</code></pre> <p>You\u2019ll see:</p> <pre><code>Please select an option:\n1. Run PDF Downloader\n2. Run Parser\nEnter your choice (1 or 2):\n</code></pre> <p>Choose 1 to start the downloader, or 2 to start the parser.</p>"},{"location":"getting-started/#running-the-downloader","title":"Running the Downloader","text":"<p>The downloader retrieves CBSL daily rice price PDFs asynchronously.</p> <pre><code>python -m rice_price_collector.downloader\n</code></pre> <p>What it does: - Fetches price report PDFs from CBSL\u2019s publications site. - Organizes them by year in the <code>data/raw/&lt;year&gt;/</code> directory. - Skips duplicates automatically.</p> <p>Example:</p> <pre><code>data/\n\u2514\u2500\u2500 raw/\n    \u251c\u2500\u2500 2023/\n    \u2502   \u251c\u2500\u2500 2023-05-01.pdf\n    \u2502   \u251c\u2500\u2500 2023-05-02.pdf\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 2024/\n        \u251c\u2500\u2500 2024-01-01.pdf\n        \u2514\u2500\u2500 ...\n</code></pre> <p>Configuration (base URL, AJAX params, output paths) lives in <code>rice_price_collector/downloader/config.py</code>.</p>"},{"location":"getting-started/#running-the-parser","title":"Running the Parser","text":"<p>Once PDFs are downloaded, run the parser to extract structured rice price data.</p> <pre><code>python -m rice_price_collector.parser\n</code></pre> <p>What it does: - Opens each PDF using your chosen backend (<code>pdfplumber</code>, <code>tabula</code>, or fallback). - Detects Pettah and Marandagahamula tables automatically. - Cleans and aligns columns, filling missing price cells. - Exports a processed CSV to <code>data/processed/</code>.</p> <p>Example output:</p> <pre><code>data/\n\u2514\u2500\u2500 processed/\n    \u251c\u2500\u2500 2023_prices.csv\n    \u251c\u2500\u2500 2024_prices.csv\n    \u2514\u2500\u2500 summary_report.csv\n</code></pre>"},{"location":"getting-started/#configuration-files","title":"Configuration Files","text":"File Description downloader/config.py Controls CBSL endpoints and output directories parser/config.py Defines paths, start/end keywords, and extraction settings parser/columns.py Defines standardized column names (e.g., \u201cRetail_Pettah\u201d, \u201cWholesale_Marandagahamula\u201d)"},{"location":"getting-started/#data-flow-summary","title":"Data Flow Summary","text":"<p>CBSL Website \u2192 Downloader \u2192 PDF Files \u2192 Parser \u2192 Clean CSV \u2192 Model-ready Dataset</p> <p>The modular design means you can run each stage independently or orchestrate both with:</p> <pre><code>python -m rice_price_collector\n</code></pre>"},{"location":"getting-started/#tip-for-developers","title":"Tip for Developers","text":"<p>Want to test just one piece?</p> <pre><code>python -m rice_price_collector.parser.extractors.rice\n</code></pre> <p>You can isolate submodules for debugging or incremental development.</p>"},{"location":"getting-started/#youre-ready","title":"You\u2019re Ready!","text":"<p>You now have the tools to: - Fetch CBSL rice price PDFs automatically - Parse and clean the extracted data - Build forecasting models on top of the generated dataset</p> <p>Next, explore:</p> <p>Downloader Module \u00bb</p> <p>Parser Module \u00bb</p>"},{"location":"pdf_downloader/","title":"PDF Downloader Module","text":"<p>The PDF Downloader is a fully asynchronous module that automatically fetches and organizes daily rice price reports from the Central Bank of Sri Lanka (CBSL) website.</p> <p>It forms the first stage of the Rice Price Collector pipeline, responsible for retrieving raw report PDFs before extraction and analysis.</p>"},{"location":"pdf_downloader/#overview","title":"Overview","text":"<ul> <li>Built on <code>aiohttp</code> + <code>asyncio</code> for concurrent, efficient downloads  </li> <li>Handles CBSL\u2019s paginated AJAX interface  </li> <li>Extracts report dates from link text to name files  </li> <li>Organizes reports automatically by year  </li> <li>Avoids duplicate downloads  </li> <li>Gracefully skips malformed or empty links  </li> </ul>"},{"location":"pdf_downloader/#how-it-works","title":"How It Works","text":"<ol> <li>The module sends async POST requests to CBSL\u2019s internal AJAX endpoint.  </li> <li>Each response contains an embedded HTML fragment listing PDF links.  </li> <li>The script extracts these links and parses dates from their titles.  </li> <li>PDFs are downloaded concurrently (five at a time by default).  </li> <li>Files are stored neatly under <code>data/raw/&lt;year&gt;/</code>.</li> </ol>"},{"location":"pdf_downloader/#example-console-output","title":"Example Console Output","text":"<p>Fetching reports for 2025 (year_id=88)... Found 24 reports on page 0 (2025) Found 24 reports on page 1 (2025) Total PDFs found for 2025: 378 Starting downloads for 2025... Saved data/raw/2025/2025-09-25.pdf Saved data/raw/2025/2025-09-26.pdf Completed downloads for 2025. Saved in data/raw/2025</p>"},{"location":"pdf_downloader/#run-the-downloader","title":"Run the Downloader","text":"<p>From your project root, run:</p> <pre><code>python -m rice_price_collector.downloader.pdf_dowloader\n</code></pre> <p>This will:</p> <ul> <li>Fetch all CBSL daily price reports for the configured years</li> <li>Create a folder per year inside data/raw/</li> <li>Download all available PDFs concurrently</li> </ul> <p>Output Folder Example</p> <pre><code>data/raw/\n\u251c\u2500\u2500 2023/\n\u2502   \u251c\u2500\u2500 2023-06-18.pdf\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 2024/\n\u2502   \u251c\u2500\u2500 2024-07-05.pdf\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 2025/\n    \u251c\u2500\u2500 2025-09-25.pdf\n    \u2514\u2500\u2500 2025-09-26.pdf\n</code></pre>"},{"location":"pdf_downloader/#key-dependencies","title":"Key Dependencies","text":"<ul> <li><code>aiohttp</code> \u2013 asynchronous HTTP client</li> <li><code>asyncio</code> \u2013 concurrency handling</li> <li><code>BeautifulSoup4</code> \u2013 HTML parsing</li> <li><code>datetime</code> \u2013 date parsing</li> <li><code>os</code> / <code>urllib.parse</code> \u2013 file and path utilities</li> </ul>"},{"location":"pdf_downloader/#notes","title":"Notes","text":"<ul> <li>Each year\u2019s CBSL reports are identified by a numeric year_id in the code (e.g. 88 for 2025).</li> <li>The downloader uses polite scraping practices \u2014 1-second pauses per page and limited concurrency.</li> <li>Any malformed or duplicate entries are automatically skipped.</li> </ul>"},{"location":"pdf_downloader/#module-location","title":"Module Location","text":"<pre><code>rice_price_collector/downloader/pdf_dowloader.py\n</code></pre>"},{"location":"api/downloader/","title":"Downloader API Reference","text":""},{"location":"api/downloader/#download_all_pdfs","title":"download_all_pdfs","text":"<pre><code>download_all_pdfs(years=None, outputdir=None)\n</code></pre> <p>Downloads all available CBSL rice price PDFs for the specified years into the given output directory. If no years or output directory are provided, prompts the user for input.</p> <p>Parameters: - <code>years</code> (list of int or str, optional): Years to download (e.g., <code>[2024, 2025]</code>). If <code>None</code>, prompts the user. - <code>outputdir</code> (str or Path, optional): Directory to save PDFs. If <code>None</code>, prompts the user.</p> <p>Usage Example:</p> <pre><code>import asyncio\nfrom rice_price_collector.downloader import download_all_pdfs\nawait download_all_pdfs([2024, 2025], './data/raw')\n</code></pre>"},{"location":"api/downloader/#download_pdfs_to","title":"download_pdfs_to","text":"<pre><code>download_pdfs_to(years, outputdir)\n</code></pre> <p>Downloads CBSL rice price PDFs for the specified years directly into the given output directory, without prompting the user.</p> <p>Parameters: - <code>years</code> (list of int or str): Years to download (e.g., <code>[2024, 2025]</code>). - <code>outputdir</code> (str or Path): Directory to save PDFs.</p> <p>Usage Example:</p> <pre><code>import asyncio\nfrom rice_price_collector.downloader import download_pdfs_to\nawait download_pdfs_to([2024, 2025], './data/raw')\n</code></pre> <p>Both functions are asynchronous and should be called with <code>await</code> inside an async function or Jupyter notebook cell.</p>"},{"location":"api/parser/","title":"Parser API Reference","text":""},{"location":"api/parser/#process_year_folders_dict","title":"process_year_folders_dict","text":"<pre><code>process_year_folders_dict(folders_dict)\n</code></pre> <p>Processes a dictionary mapping years to folders containing CBSL rice price PDFs, extracting data from all PDFs and returning a combined pandas DataFrame.</p> <p>Parameters: - <code>folders_dict</code> (dict): Mapping of year (str or int) to folder path (str or Path) containing PDFs. Example: <code>{ \"2024\": \"./data/raw/2024\" }</code></p> <p>Returns: - <code>pandas.DataFrame</code> or <code>None</code>: Combined extracted data, or <code>None</code> if no data found.</p> <p>Usage Example:</p> <pre><code>from rice_price_collector.parser import process_year_folders_dict\nfolders = {\"2024\": \"./data/raw/2024\", \"2025\": \"./data/raw/2025\"}\ndf = process_year_folders_dict(folders)\n</code></pre>"},{"location":"api/parser/#parse_price_section","title":"parse_price_section","text":"<pre><code>parse_price_section(section_lines)\n</code></pre> <p>Parses a section of lines from a rice price PDF and extracts structured data.</p> <p>Parameters: - <code>section_lines</code> (list of str): Lines of text from a PDF section.</p> <p>Returns: - <code>dict</code>: Extracted data from the section.</p>"},{"location":"api/parser/#create_smart_column_names","title":"create_smart_column_names","text":"<pre><code>create_smart_column_names(header_lines)\n</code></pre> <p>Generates smart column names from header lines in a PDF table.</p> <p>Parameters: - <code>header_lines</code> (list of str): Lines representing table headers.</p> <p>Returns: - <code>list of str</code>: List of column names.</p>"},{"location":"api/parser/#extract_section_between","title":"extract_section_between","text":"<pre><code>extract_section_between(lines, start_marker, end_marker)\n</code></pre> <p>Extracts lines between two markers in a list of lines.</p> <p>Parameters: - <code>lines</code> (list of str): All lines from a PDF or text. - <code>start_marker</code> (str): Start marker. - <code>end_marker</code> (str): End marker.</p> <p>Returns: - <code>list of str</code>: Extracted lines.</p>"},{"location":"api/parser/#fix_missing_columns","title":"fix_missing_columns","text":"<pre><code>fix_missing_columns(df)\n</code></pre> <p>Fixes missing columns in a DataFrame extracted from a PDF.</p> <p>Parameters: - <code>df</code> (pandas.DataFrame): DataFrame to fix.</p> <p>Returns: - <code>pandas.DataFrame</code>: Fixed DataFrame.</p>"},{"location":"downloader/","title":"Downloader Module Overview","text":"<p>The Downloader Module is responsible for automatically fetching daily rice price PDF reports from the Central Bank of Sri Lanka (CBSL) website. It uses asynchronous HTTP requests to efficiently download hundreds of reports across multiple years, ensuring a consistent and up-to-date local dataset for parsing.</p>"},{"location":"downloader/#purpose","title":"Purpose","text":"<p>The CBSL publishes daily \u201cPrice Reports\u201d containing retail and wholesale rice prices for Pettah and Marandagahamula. Manually saving these PDFs is tedious \u2014 so this module automates that process.</p> <p>The downloader: - Crawls CBSL\u2019s price publications endpoint. - Paginates through all available reports. - Extracts publication dates and direct PDF URLs. - Downloads and saves them to structured folders (e.g., <code>data/raw/2023/2023-05-14.pdf</code>).</p>"},{"location":"downloader/#key-features","title":"Key Features","text":"Feature Description Asynchronous downloading Built on <code>aiohttp</code>, allowing many reports to be fetched in parallel. Dynamic pagination Automatically traverses CBSL\u2019s AJAX-based pages until no new PDFs are found. Resumable runs Skips already downloaded files to avoid duplication. Robust error handling Retries failed requests and logs warnings for missing or invalid URLs. Configurable base URLs Controlled entirely via <code>config.py</code>, so adapting to new CBSL endpoints is simple. Year-wise output directories PDFs are neatly organized by publication year for easier versioning and parsing."},{"location":"downloader/#module-structure","title":"Module Structure","text":"<pre><code>rice_price_collector/downloader/\n\u251c\u2500\u2500 init.py\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 config.py \u2190 CBSL URLs and output directories\n\u2514\u2500\u2500 pdf_downloader.py \u2190 Core logic: fetch_page(), scrape_all(), save_pdf()\n</code></pre>"},{"location":"downloader/#configuration-configpy","title":"Configuration (<code>config.py</code>)","text":"<p>The configuration file defines the key constants and directories used by the module:</p> <pre><code>BASE = \"https://www.cbsl.gov.lk/publications/price\"\nAJAX_URL = \"https://www.cbsl.gov.lk/views/ajax\"\nOUTPUT_DIR = \"data/raw\"\n</code></pre> <p>You can update: - BASE to match the CBSL \u201cPrice Reports\u201d main page. - AJAX_URL if CBSL changes their internal data request endpoint. - OUTPUT_DIR to change where files are saved.</p>"},{"location":"downloader/#how-it-works","title":"How It Works","text":"<p>Initialize session The module starts an aiohttp.ClientSession for efficient reuse of HTTP connections.</p> <p>Fetch paginated HTML It posts to the CBSL AJAX endpoint, retrieving a block of price report listings (10\u201315 reports per page).</p> <p>Parse PDF links Using BeautifulSoup, the module extracts all PDF URLs and their publication dates.</p> <p>Download PDFs concurrently All found links are downloaded in parallel using asyncio tasks.</p> <p>Save with structured filenames Each PDF is named by its date (e.g., 2023-08-15.pdf) and stored in data/raw//."},{"location":"downloader/#example-usage","title":"Example Usage","text":"<p>Run interactively through the package menu</p> <pre><code>python -m rice_price_collector\n</code></pre> <p>\u2192 Choose option 1 for PDF Downloader.</p> <p>Run directly</p> <pre><code>python -m rice_price_collector.downloader\n</code></pre> <p>Run manually in code</p> <pre><code>from rice_price_collector.downloader.pdf_downloader import scrape_all\nimport asyncio\n\nasyncio.run(scrape_all(year_id=\"88\"))  # e.g. year 2025\n</code></pre>"},{"location":"downloader/#example-console-output","title":"Example Console Output","text":"<pre><code>[001/239] Downloading 2022-08-15.pdf... OK\n[002/239] Downloading 2022-08-16.pdf... OK\n[003/239] Downloading 2022-08-17.pdf... SKIPPED (already exists)\n[004/239] Downloading 2022-08-18.pdf... OK\n</code></pre> <p>At the end:</p> <p>Completed! 236 PDFs downloaded successfully.</p>"},{"location":"downloader/#integration-with-parser-module","title":"Integration with Parser Module","text":"<p>Once the PDFs are downloaded, the Parser Module takes over \u2014 extracting structured rice price tables. The downloader and parser are independent but work best when chained:</p> <pre><code>python -m rice_price_collector.downloader &amp;&amp; python -m rice_price_collector.parser\n</code></pre>"},{"location":"downloader/#next-step","title":"Next Step","text":"<p>Proceed to the next section to explore the parser pipeline:</p> <p>Downloader Configuration \u00bb</p>"},{"location":"downloader/config/","title":"Downloader Configuration","text":"<p>The downloader\u2019s behavior is entirely controlled through <code>rice_price_collector/downloader/config.py</code>. This file defines the base URLs, output directories, and year identifiers required to fetch CBSL price report PDFs.</p>"},{"location":"downloader/config/#location","title":"Location","text":"<pre><code>rice_price_collector/\n\u2514\u2500\u2500 downloader/\n    \u2514\u2500\u2500 config.py\n</code></pre>"},{"location":"downloader/config/#overview","title":"Overview","text":"<p>The configuration file provides three categories of settings:</p> <ol> <li>Endpoints \u2013 URLs used to access CBSL\u2019s price report listings  </li> <li>Output paths \u2013 Where to store downloaded PDFs  </li> <li>Runtime parameters \u2013 Defaults like timeouts, concurrency, and year filters  </li> </ol>"},{"location":"downloader/config/#endpoint-settings","title":"Endpoint Settings","text":"<pre><code># Base CBSL publication page for daily price reports\nBASE = \"https://www.cbsl.gov.lk/publications/price\"\n\n# AJAX endpoint used by CBSL\u2019s internal Drupal views to load paginated content\nAJAX_URL = \"https://www.cbsl.gov.lk/views/ajax\"\n</code></pre> <p>What these do:</p> <p>BASE is the visible \u201cPrice Reports\u201d page users can access in a browser.</p> <p>AJAX_URL is the hidden request endpoint used to fetch each page of PDFs asynchronously. The downloader posts a JSON payload to this endpoint (e.g., page number, year ID) and parses the returned HTML block.</p> <p>If CBSL changes their website layout or endpoint, update these two constants first.</p>"},{"location":"downloader/config/#output-paths","title":"Output Paths","text":"<pre><code># Folder where PDFs will be stored (created automatically if not found)\nOUTPUT_DIR = \"data/raw\"\n</code></pre> <p>Each year\u2019s reports are saved in a subfolder:</p> <pre><code>data/\n\u2514\u2500\u2500 raw/\n    \u251c\u2500\u2500 2023/\n    \u2502   \u251c\u2500\u2500 2023-05-01.pdf\n    \u2502   \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 2024/\n        \u251c\u2500\u2500 2024-01-01.pdf\n        \u2514\u2500\u2500 ...\n</code></pre> <p>You can change this directory to any path of your choice \u2014 relative or absolute:</p> <pre><code>OUTPUT_DIR = \"/mnt/storage/cbsl_pdfs\"\n</code></pre>"},{"location":"downloader/config/#year-identifiers","title":"Year Identifiers","text":"<p>CBSL organizes publications by internal \u201cyear IDs.\u201d These IDs don\u2019t always match the calendar year directly \u2014 for example:</p> <ul> <li>88 \u2192 2025</li> <li>87 \u2192 2024</li> <li>86 \u2192 2023</li> </ul> <p>To download reports for multiple years, simply call the downloader multiple times:</p> <pre><code>from rice_price_collector.downloader.pdf_downloader import scrape_all\nimport asyncio\n\nfor year_id in [\"86\", \"87\", \"88\"]:\n    asyncio.run(scrape_all(year_id))\n</code></pre> <p>You can also parameterize this inside your script or CLI wrapper.</p>"},{"location":"downloader/config/#runtime-parameters-optional","title":"Runtime Parameters (optional)","text":"<p>You can add or tune extra parameters to control runtime behavior:</p> <pre><code># Maximum number of concurrent downloads\nMAX_CONCURRENT = 10\n\n# Request timeout (seconds)\nREQUEST_TIMEOUT = 30\n\n# Number of retry attempts for failed downloads\nRETRY_LIMIT = 3\n\n# Whether to overwrite existing PDFs\nOVERWRITE = False\n</code></pre> <p>These are optional defaults \u2014 not all may exist in your initial version, but defining them helps make the downloader resilient to flaky connections.</p>"},{"location":"downloader/config/#example-payload-behind-the-scenes","title":"Example Payload (behind the scenes)","text":"<p>When requesting a page, the downloader sends a POST request to AJAX_URL like this:</p> <pre><code>payload = {\n    \"view_name\": \"price_report\",\n    \"view_display_id\": \"block_1\",\n    \"view_path\": \"node/144\",\n    \"view_base_path\": \"publications/price\",\n    \"pager_element\": 0,\n    \"page\": 3,            # current page number (0-indexed)\n    \"year\": \"88\",         # CBSL year ID for 2025\n}\n</code></pre> <p>This returns a block of HTML containing the links to PDF files for that page, which is then parsed by BeautifulSoup.</p>"},{"location":"downloader/config/#example-configuration-summary","title":"Example Configuration Summary","text":"Variable Type Description Example BASE str CBSL public \u201cPrice Reports\u201d page \"https://www.cbsl.gov.lk/publications/price\" AJAX_URL str Hidden AJAX endpoint for fetching report pages \"https://www.cbsl.gov.lk/views/ajax\" OUTPUT_DIR str Local directory where PDFs are stored \"data/raw\" MAX_CONCURRENT int Maximum parallel downloads 10 REQUEST_TIMEOUT int Timeout per request in seconds 30 RETRY_LIMIT int Number of retries for failed requests 3 OVERWRITE bool Whether to overwrite existing files False"},{"location":"downloader/config/#next-step","title":"Next Step","text":"<p>Once your configuration is set, continue to the detailed downloader implementation:</p> <p>PDF Downloader \u00bb</p>"},{"location":"downloader/downloader/","title":"PDF Downloader","text":"<p>The PDF Downloader is the heart of the Downloader module. It fetches, parses, and downloads CBSL \u201cPrice Report\u201d PDFs asynchronously using <code>aiohttp</code> and <code>BeautifulSoup</code>.</p>"},{"location":"downloader/downloader/#file-location","title":"File Location","text":"<pre><code>rice_price_collector/\n\u2514\u2500\u2500 downloader/\n    \u2514\u2500\u2500 pdf_downloader.py\n</code></pre> <p>This script can be run directly or imported as part of a larger pipeline.</p>"},{"location":"downloader/downloader/#purpose","title":"Purpose","text":"<p>CBSL publishes daily price reports as PDFs. Each year has hundreds of files, spread across multiple AJAX-driven pages. The downloader automates:</p> <ol> <li>Querying the CBSL AJAX endpoint  </li> <li>Extracting PDF URLs and publication dates  </li> <li>Downloading and saving them by year  </li> </ol> <p>It\u2019s designed for speed, reliability, and reusability.</p>"},{"location":"downloader/downloader/#main-functions","title":"Main Functions","text":""},{"location":"downloader/downloader/#1-fetch_pagesession-page_num-year_id","title":"1. <code>fetch_page(session, page_num, year_id)</code>","text":"<p>Fetches one page of PDF listings from the CBSL AJAX endpoint.</p> <pre><code>async def fetch_page(session, page_num, year_id):\n    \"\"\"\n    Fetch a single page of PDF links from the CBSL website.\n\n    Args:\n        session (aiohttp.ClientSession): The HTTP session to use for requests\n        page_num (int): The page number to fetch (0-indexed)\n        year_id (str): The CBSL year ID filter (e.g., \"88\" for 2025)\n\n    Returns:\n        list: A list of tuples containing (date_object, pdf_url)\n    \"\"\"\n</code></pre> <p>What it does:</p> <ul> <li>Posts a JSON payload to AJAX_URL (defined in config.py).</li> <li>Parses the HTML response using BeautifulSoup.</li> <li>Extracts each PDF\u2019s URL and publication date.</li> <li>Returns them as a Python list.</li> <li>If a request fails, it retries or returns an empty list with a warning.</li> </ul>"},{"location":"downloader/downloader/#2-download_pdfsession-url-dest_path","title":"2. <code>download_pdf(session, url, dest_path)</code>","text":"<p>Downloads a single PDF asynchronously and writes it to disk.</p> <pre><code>async def download_pdf(session, url, dest_path):\n    \"\"\"\n    Download a PDF and save it to disk.\n\n    Args:\n        session (aiohttp.ClientSession): Active HTTP session\n        url (str): Full URL to the PDF file\n        dest_path (str): Local file path to save\n    \"\"\"\n</code></pre> <ul> <li>Checks if the file already exists \u2014 unless OVERWRITE = True in config \u2014 and creates directories automatically.</li> <li>Any failed downloads are logged with a retry message.</li> </ul>"},{"location":"downloader/downloader/#4-main-or-__main__-entry","title":"4. <code>main()</code> or <code>__main__</code> entry","text":"<p>This allows running the downloader directly from the command line:</p> <pre><code>python -m rice_price_collector.downloader\n</code></pre> <p>Internally, it calls scrape_all() for the configured year_id (which you can modify in config.py).</p>"},{"location":"downloader/downloader/#example-run","title":"Example Run","text":"<pre><code>python -m rice_price_collector.downloader\n</code></pre> <p>Output:</p> <p>Starting CBSL PDF scrape for 2025 (year_id=88)... [001/239] Downloading 2025-01-02.pdf... OK [002/239] Downloading 2025-01-03.pdf... OK [003/239] Downloading 2025-01-04.pdf... SKIPPED (already exists) Completed! 236 PDFs downloaded successfully.</p>"},{"location":"downloader/downloader/#example-programmatic-usage","title":"Example Programmatic Usage","text":"<p>You can also import and run it from your own scripts:</p> <pre><code>import asyncio\nfrom rice_price_collector.downloader.pdf_downloader import scrape_all\n\nasyncio.run(scrape_all(\"88\"))  # Year 2025\n</code></pre> <p>To scrape multiple years in one run:</p> <pre><code>import asyncio\nfrom rice_price_collector.downloader.pdf_downloader import scrape_all\n\nasync def run_all():\n    await asyncio.gather(\n        scrape_all(\"86\"),  # 2023\n        scrape_all(\"87\"),  # 2024\n        scrape_all(\"88\"),  # 2025\n    )\n\nasyncio.run(run_all())\n</code></pre>"},{"location":"downloader/downloader/#logging-and-progress-output","title":"Logging and Progress Output","text":"<p>Each file download is numbered and timestamped:</p> <p>[145/239] Downloading 2022-08-12.pdf... OK [146/239] Downloading 2022-08-13.pdf... OK [147/239] Downloading 2022-08-14.pdf... WARNING: failed, retrying... [147/239] Downloading 2022-08-14.pdf... OK</p> <p>All warnings and errors are printed directly to the console. You can extend this with the logging module if you prefer file logs.</p>"},{"location":"downloader/downloader/#error-handling","title":"Error Handling","text":"<p>The downloader is designed to fail gracefully.</p> Scenario Behavior Network timeout Retries up to RETRY_LIMIT times 404 / Missing PDF Logs a warning and skips file Duplicate filename Skips unless OVERWRITE=True Invalid year_id Exits cleanly after zero results"},{"location":"downloader/downloader/#performance-notes","title":"Performance Notes","text":"<p>Each page fetch returns 10\u201315 PDFs; typical full-year runs involve ~250\u2013300 files.</p> <p>With 10 concurrent connections, a full year downloads in ~30\u201340 seconds on a good network.</p> <p>If CBSL rate-limits requests, reduce concurrency in config.py (MAX_CONCURRENT).</p>"},{"location":"downloader/downloader/#developer-tips","title":"Developer Tips","text":"<ul> <li>Use asyncio.Semaphore to limit active downloads.</li> <li>Set session.timeout = aiohttp.ClientTimeout(total=60) to avoid hanging connections.</li> <li>Add headers like User-Agent if CBSL starts blocking generic requests.</li> </ul> <p>Example snippet:</p> <pre><code>headers = {\"User-Agent\": \"rice-price-collector/1.0\"}\nasync with aiohttp.ClientSession(headers=headers) as session:\n    ...\n</code></pre>"},{"location":"downloader/downloader/#next-step","title":"Next Step","text":"<p>Once you\u2019ve downloaded all PDFs, head to the Parser Module to start extracting tables:</p> <p>Parser Overview \u00bb</p>"},{"location":"parser/","title":"Parser Module Overview","text":"<p>The Parser module is responsible for extracting, cleaning, and structuring rice price data from CBSL price report PDFs. It transforms raw, unstructured text into clean, analysis-ready tables using a modular, extensible pipeline.</p>"},{"location":"parser/#purpose","title":"Purpose","text":"<p>CBSL price reports contain rice price tables embedded in PDFs, with inconsistent formatting and occasional missing data. The Parser automates: - Locating and extracting the relevant table section from each PDF - Parsing messy text into structured rows and columns - Cleaning, aligning, and labeling columns for downstream analysis - Batch processing of entire year folders for large-scale data extraction</p>"},{"location":"parser/#key-features","title":"Key Features","text":"Feature Description Section extraction Finds and extracts the \u201cRICE\u201d section from each PDF using keyword boundaries Smart column naming Dynamically generates column names based on detected markets and days Data cleaning utilities Fixes missing columns, merges broken lines, and standardizes units Batch processing Processes all PDFs in a year folder and outputs combined CSVs Extensible extractors Easily add new extractors for other commodities or table types"},{"location":"parser/#module-structure","title":"Module Structure","text":"<pre><code>rice_price_collector/parser/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 batch_extract.py      # Batch extraction for multiple years\n\u251c\u2500\u2500 columns.py            # Smart column naming logic\n\u251c\u2500\u2500 config.py             # Extraction and cleaning settings\n\u251c\u2500\u2500 extractors/\n\u2502   \u2514\u2500\u2500 rice.py           # Main rice table extractor\n\u251c\u2500\u2500 parser.py             # Core parsing logic for price tables\n\u2514\u2500\u2500 utils.py              # PDF section extraction and helpers\n</code></pre>"},{"location":"parser/#main-components","title":"Main Components","text":"<ul> <li>extract_and_parse_rice (extractors/rice.py): Extracts and parses the rice price table from a PDF page.</li> <li>parse_price_section (parser.py): Cleans and splits raw text into a DataFrame.</li> <li>create_smart_column_names (columns.py): Generates meaningful column names based on detected markets.</li> <li>extract_section_between (utils.py): Extracts text between keywords in a PDF page.</li> <li>fix_missing_columns (utils.py): Ensures all rows have the correct number of columns.</li> <li>process_year_folder (batch_extract.py): Processes all PDFs in a year folder and saves combined CSVs.</li> </ul>"},{"location":"parser/#example-usage","title":"Example Usage","text":"<p>Batch extract all years:</p> <pre><code>python -m rice_price_collector.parser.batch_extract 2025 2024 2023\n</code></pre> <p>Extract and parse a single PDF:</p> <pre><code>from rice_price_collector.parser.extractors.rice import extract_and_parse_rice\n\ndf = extract_and_parse_rice(\"data/raw/2025/2025-01-01.pdf\")\nprint(df.head())\n</code></pre>"},{"location":"parser/#data-flow","title":"Data Flow","text":"<p>PDF file \u2192 Extract section \u2192 Parse text \u2192 Clean columns \u2192 Output DataFrame/CSV</p>"},{"location":"parser/#next-step","title":"Next Step","text":"<p>Continue to the batch extractor, rice extractor, or configuration documentation for more details on each component.</p> <p>Batch Extractor \u00bb</p>"},{"location":"parser/batch_extract/","title":"Batch Extractor","text":"<p>The batch extractor automates the extraction of rice price tables from all CBSL PDF reports in one or more yearly folders. It processes each PDF, parses the \"RICE\" section, and saves the results as combined CSV files for each year.</p>"},{"location":"parser/batch_extract/#purpose","title":"Purpose","text":"<p>CBSL releases hundreds of daily price report PDFs per year. Manually extracting tables from each file is tedious and error-prone. The batch extractor: - Scans a folder of PDFs for a given year - Extracts and parses the \"RICE\" section from each file - Combines all results into a single DataFrame - Saves the output as a CSV for each year - Optionally, combines multiple years for further analysis</p>"},{"location":"parser/batch_extract/#file-location","title":"File Location","text":"<p>rice_price_collector/ \u2514\u2500\u2500 parser/     \u2514\u2500\u2500 batch_extract.py</p>"},{"location":"parser/batch_extract/#usage","title":"Usage","text":"<p>Run from the command line to process one or more years:</p> <pre><code>python -m rice_price_collector.parser.batch_extract 2025 2024 2023\n</code></pre> <p>This will look inside: - ../data/raw/2025/ - ../data/raw/2024/ - ../data/raw/2023/</p> <p>and extract the \"RICE\" section from every PDF found in each folder. Each year's results are saved as CSV, and a combined file can be created for all years.</p>"},{"location":"parser/batch_extract/#main-function-process_year_folder","title":"Main Function: process_year_folder","text":"<pre><code>def process_year_folder(year_folder: Path, output_dir: Path):\n    \"\"\"\n    Process all PDFs within a given year's folder and save the combined CSV.\n    \"\"\"\n    # ...\n</code></pre> <ul> <li>Iterates through all PDFs in the specified year folder</li> <li>Extracts and parses the \"RICE\" section using <code>extract_and_parse_rice</code></li> <li>Adds the date (from filename) to each row</li> <li>Combines all DataFrames and saves as <code>rice_prices_&lt;year&gt;.csv</code></li> <li>Prints progress and error messages for each file</li> </ul>"},{"location":"parser/batch_extract/#example-output","title":"Example Output","text":"<pre><code>[1/239] \u2192 2025-01-01.pdf\n[2/239] \u2192 2025-01-02.pdf\n...\nSaved 236 rows \u2192 /absolute/path/to/rice_prices_2025.csv\n</code></pre>"},{"location":"parser/batch_extract/#error-handling","title":"Error Handling","text":"<ul> <li>Skips files with no \"RICE\" section or empty data</li> <li>Prints a message for any PDF that fails to parse</li> <li>Continues processing remaining files even if some fail</li> </ul>"},{"location":"parser/batch_extract/#extending","title":"Extending","text":"<ul> <li>You can modify the script to process other sections (e.g., \"FISH\") by changing the extractor</li> <li>Combine multiple years' CSVs for time series analysis</li> <li>Integrate with the downloader for a full pipeline</li> </ul>"},{"location":"parser/batch_extract/#next-step","title":"Next Step","text":"<p>Proceed to the rice extractor, parser logic, or configuration documentation for more details on each component.</p> <p> Rice Extractor \u00bb</p>"},{"location":"parser/columns/","title":"Column Naming Logic","text":"<p>The column naming logic in the parser ensures that extracted rice price tables have clear, meaningful column names that reflect the market, price type, and day. This makes the data easy to analyze and avoids generic names like \"Col1\", \"Col2\", etc.</p>"},{"location":"parser/columns/#purpose","title":"Purpose","text":"<p>CBSL price tables report prices for multiple markets and days. Instead of ambiguous column names, we generate descriptive names such as <code>wholesale_pettah_yesterday</code> or <code>retail_dambulla_today</code>.</p>"},{"location":"parser/columns/#how-column-names-are-created","title":"How Column Names Are Created","text":"<p>The function <code>create_smart_column_names(section_lines)</code>:</p> <ol> <li>Scans the first 50 lines of the extracted section to determine which markets are present (e.g., Pettah, Marandagahamula, Dambulla, Narahenpita).</li> <li>Defines default market lists for wholesale and retail prices.</li> <li>Builds column names for each market and day (yesterday/today) for both wholesale and retail prices.</li> <li>Returns a list of column names in the order they appear in the table.</li> </ol>"},{"location":"parser/columns/#example-output","title":"Example Output","text":"<p>A typical output might look like:</p> <pre><code>[\"item\", \"unit\",\n \"wholesale_pettah_yesterday\", \"wholesale_pettah_today\",\n \"wholesale_marandagahamula_yesterday\", \"wholesale_marandagahamula_today\",\n \"retail_pettah_yesterday\", \"retail_pettah_today\",\n \"retail_dambulla_yesterday\", \"retail_dambulla_today\",\n \"retail_narahenpita_yesterday\", \"retail_narahenpita_today\"]\n</code></pre>"},{"location":"parser/columns/#customization","title":"Customization","text":"<ul> <li>The function can be extended to detect additional markets or handle new table formats.</li> <li>If a market is not present in the data, you can adjust the logic to skip or add columns as needed.</li> </ul>"},{"location":"parser/columns/#code-reference","title":"Code Reference","text":"<pre><code>def create_smart_column_names(section_lines):\n    # ...\n    wholesale_locations = [\"pettah\", \"marandagahamula\"]\n    retail_locations = [\"pettah\", \"dambulla\", \"narahenpita\"]\n    # ...\n    for location in wholesale_locations:\n        column_names.append(f\"wholesale_{location}_yesterday\")\n        column_names.append(f\"wholesale_{location}_today\")\n    for location in retail_locations:\n        column_names.append(f\"retail_{location}_yesterday\")\n        column_names.append(f\"retail_{location}_today\")\n    return column_names\n</code></pre>"},{"location":"parser/columns/#next-step","title":"Next Step","text":"<p>See the parser or extractor documentation for how these column names are used in the final DataFrame.</p>"},{"location":"parser/config/","title":"Parser Configuration","text":"<p>The parser\u2019s behavior is controlled through <code>rice_price_collector/parser/config.py</code> and the shared project config in <code>rice_price_collector/config.py</code>. These files define the keywords, page numbers, column handling, and data directories used for extracting and processing CBSL rice price tables.</p>"},{"location":"parser/config/#location","title":"Location","text":"<pre><code>rice_price_collector/\n\u251c\u2500\u2500 config.py                # Project-wide directories\n\u2514\u2500\u2500 parser/\n    \u2514\u2500\u2500 config.py           # Parser-specific settings\n</code></pre>"},{"location":"parser/config/#overview","title":"Overview","text":"<p>The configuration files provide:</p> <ol> <li>Data directories \u2013 Where to find raw PDFs and save processed CSVs</li> <li>Section extraction settings \u2013 Keywords and page numbers for table extraction</li> <li>Column handling \u2013 Number and position of price columns</li> <li>File naming \u2013 Timestamp format for output files</li> </ol>"},{"location":"parser/config/#data-directories","title":"Data Directories","text":"<p>Defined in <code>rice_price_collector/config.py</code>:</p> <pre><code>from pathlib import Path\n\nBASE_DIR = Path(__file__).resolve().parents[1]\nRAW_DATA_DIR = BASE_DIR / \"data\" / \"raw\"\nPROCESSED_DIR = BASE_DIR / \"data\" / \"processed\"\n\n# Ensure directories exist or path in [RAW_DATA_DIR, PROCESSED_DIR]:\n    path.mkdir(parents=True, exist_ok=True)\n</code></pre> <ul> <li><code>RAW_DATA_DIR</code>: Where input PDFs are stored (organized by year)</li> <li><code>PROCESSED_DIR</code>: Where cleaned CSVs and batch outputs are saved</li> </ul>"},{"location":"parser/config/#section-extraction-settings","title":"Section Extraction Settings","text":"<p>Defined in <code>rice_price_collector/parser/config.py</code>:</p> <pre><code>DEFAULT_START_WORD = \"RICE\"\nDEFAULT_END_WORD = \"FISH\"\nDEFAULT_PAGE_NUMBER = 2\n</code></pre> <ul> <li><code>DEFAULT_START_WORD</code>: Keyword marking the start of the rice price table</li> <li><code>DEFAULT_END_WORD</code>: Keyword marking the end of the section</li> <li><code>DEFAULT_PAGE_NUMBER</code>: Which page to extract from (usually 2)</li> </ul>"},{"location":"parser/config/#column-handling","title":"Column Handling","text":"<pre><code>TOTAL_PRICE_COLUMNS = 10\nINSERT_POSITION = 6\n</code></pre> <ul> <li><code>TOTAL_PRICE_COLUMNS</code>: Expected number of price columns in the table</li> <li><code>INSERT_POSITION</code>: Where to insert missing columns if data is incomplete</li> </ul>"},{"location":"parser/config/#file-naming","title":"File Naming","text":"<pre><code>TIMESTAMP_FORMAT = \"%Y-%m-%d_%H-%M-%S\"\n</code></pre> <ul> <li><code>TIMESTAMP_FORMAT</code>: Used for naming output files with timestamps</li> </ul>"},{"location":"parser/config/#good-practices","title":"Good Practices","text":"<ul> <li>Keep config.py versioned to track changes in extraction logic or CBSL report structure</li> <li>Adjust keywords and page numbers if CBSL changes their PDF format</li> <li>Use relative paths for portability</li> </ul>"},{"location":"parser/config/#next-step","title":"Next Step","text":"<p>Continue to the batch extractor, rice extractor, or parser implementation for more details on how these settings are used.</p>"},{"location":"parser/utils/","title":"Parser Utilities","text":"<p>The utilities in the parser module provide essential helper functions for extracting and cleaning rice price data from CBSL PDFs. They handle PDF text extraction and ensure data consistency in the resulting tables.</p>"},{"location":"parser/utils/#purpose","title":"Purpose","text":"<p>CBSL price tables are embedded in PDFs with inconsistent formatting and sometimes missing data. The utilities: - Extract text between specific keywords in a PDF page - Ensure all rows in the parsed table have the correct number of columns</p>"},{"location":"parser/utils/#main-functions","title":"Main Functions","text":""},{"location":"parser/utils/#1-extract_section_between","title":"1. extract_section_between","text":"<p>Extracts all text between two keywords (e.g., \"RICE\" and \"FISH\") from a specific page of a PDF.</p> <pre><code>def extract_section_between(pdf_path, start_letters, end_letters, page_num=2):\n    # ...\n</code></pre> <ul> <li>Uses <code>pdfplumber</code> to read the PDF and group text by line position</li> <li>Finds the y-coordinates of the start and end keywords</li> <li>Returns all lines of text between those positions</li> <li>Handles cases where keywords are spaced out (e.g., \"R I C E\")</li> </ul> <p>Typical usage: - Isolate the rice price table from a multi-table PDF</p>"},{"location":"parser/utils/#2-fix_missing_columns","title":"2. fix_missing_columns","text":"<p>Ensures each row in the price table has the expected number of columns, filling in missing values as needed.</p> <pre><code>def fix_missing_columns(price_list, total_columns=10, insert_position=6):\n    # ...\n</code></pre> <ul> <li>Checks if a row has fewer columns than expected</li> <li>Inserts <code>None</code> values at the specified position to pad the row</li> <li>Returns a list with exactly <code>total_columns</code> items</li> </ul> <p>Why? - Some rows may be missing prices (e.g., missing retail prices) - Ensures the DataFrame is rectangular and ready for analysis</p>"},{"location":"parser/utils/#example-usage","title":"Example Usage","text":"<pre><code>section = extract_section_between(\"data/raw/2025/2025-01-01.pdf\", \"RICE\", \"FISH\", page_num=2)\nrow = fix_missing_columns([130.00, 132.00, 135.00], total_columns=10, insert_position=6)\n</code></pre>"},{"location":"parser/utils/#extending","title":"Extending","text":"<ul> <li>Adjust the keyword logic in <code>extract_section_between</code> to support new table formats</li> <li>Change <code>total_columns</code> or <code>insert_position</code> in <code>fix_missing_columns</code> for different data layouts</li> </ul>"},{"location":"parser/utils/#next-step","title":"Next Step","text":"<p>See the extractor, parser, or batch extraction documentation for how these utilities are used in the full pipeline.</p> <p>Configuration \u00bb</p>"},{"location":"parser/extractors/rice/","title":"Rice Extractor","text":"<p>The rice extractor is a specialized function for extracting and parsing the \"RICE\" price table from CBSL price report PDFs. It locates the relevant section, parses the text into a structured DataFrame, and assigns meaningful column names based on the detected markets and days.</p>"},{"location":"parser/extractors/rice/#purpose","title":"Purpose","text":"<p>CBSL price report PDFs contain multiple tables for different commodities. The rice extractor automates: - Locating the \"RICE\" section within a PDF page - Parsing the messy text into a clean, tabular DataFrame - Assigning smart column names for easy analysis</p>"},{"location":"parser/extractors/rice/#file-location","title":"File Location","text":"<p>rice_price_collector/ \u2514\u2500\u2500 parser/     \u2514\u2500\u2500 extractors/         \u2514\u2500\u2500 rice.py</p>"},{"location":"parser/extractors/rice/#main-function-extract_and_parse_rice","title":"Main Function: extract_and_parse_rice","text":"<pre><code>def extract_and_parse_rice(pdf_path, start_word=\"RICE\", end_word=\"FISH\", page_number=2):\n    \"\"\"\n    Extracts the RICE section from a PDF and converts it to a table with smart column names.\n    \"\"\"\n    # ...\n</code></pre>"},{"location":"parser/extractors/rice/#what-it-does","title":"What it does:","text":"<ol> <li>Uses <code>extract_section_between</code> to find all text between the start and end keywords (default: \"RICE\" to \"FISH\") on the specified page (default: 2).</li> <li>Parses the extracted lines into a DataFrame using <code>parse_price_section</code>.</li> <li>Generates meaningful column names with <code>create_smart_column_names</code> and assigns them to the DataFrame.</li> </ol>"},{"location":"parser/extractors/rice/#parameters","title":"Parameters","text":"<ul> <li><code>pdf_path</code>: Path to the PDF file</li> <li><code>start_word</code>: Section start keyword (default: \"RICE\")</li> <li><code>end_word</code>: Section end keyword (default: \"FISH\")</li> <li><code>page_number</code>: Page number to extract from (default: 2)</li> </ul>"},{"location":"parser/extractors/rice/#returns","title":"Returns","text":"<p>A pandas DataFrame containing the parsed rice price data with smart column names.</p>"},{"location":"parser/extractors/rice/#example-usage","title":"Example Usage","text":"<pre><code>from rice_price_collector.parser.extractors.rice import extract_and_parse_rice\n\ndf = extract_and_parse_rice(\"data/raw/2025/2025-01-01.pdf\")\nprint(df.head())\n</code></pre>"},{"location":"parser/extractors/rice/#extending","title":"Extending","text":"<ul> <li>Change <code>start_word</code> and <code>end_word</code> to extract other sections (e.g., \"FISH\")</li> <li>Adjust <code>page_number</code> if the table appears on a different page</li> <li>Integrate with batch extraction to process entire year folders</li> </ul>"},{"location":"parser/extractors/rice/#next-step","title":"Next Step","text":"<p>See the parser logic, batch extractor, or configuration documentation for more details on the full extraction pipeline.</p> <p>Columns \u00bb</p>"}]}